{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e6110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and API keys loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Load API Keys\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import ast\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "import time\n",
    "import openai\n",
    "import tiktoken\n",
    "import pinecone\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = \"/Users/lokesh/Desktop/Model_Earth/rag-pipeline\"\n",
    "INDEX_NAME = \"model-earth-jam-stack\"\n",
    "INDEX_DIMENSION = 1536\n",
    "PINECONE_REGION = \"us-east-1\"\n",
    "MAX_LINES_PREVIEW = 5\n",
    "\n",
    "TOKEN_LIMIT = 800\n",
    "\n",
    "# Supported extensions\n",
    "code_exts = {\".py\", \".js\", \".ts\", \".jsx\", \".tsx\", \".java\", \".cpp\", \".c\", \".cs\", \".go\", \".rb\", \".php\", \".rs\", \".swift\",\".kt\", \".kts\", \".scala\", \".cjs\", \".mjs\", \".ipynb\", \".sh\"}\n",
    "markdown_exts = {\".md\", \".mdx\", \".txt\", \".rst\", \".adoc\"}\n",
    "data_exts = {\".csv\", \".tsv\", \".xls\", \".xlsx\", \".parquet\", \".feather\", \".pkl\"}\n",
    "json_exts = {\".json\", \".yaml\", \".yml\", \".jsonl\", \".webmanifest\"}\n",
    "image_exts = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".psd\", \".bmp\", \".tiff\", \".ico\"}\n",
    "font_exts = {\".woff\", \".woff2\", \".ttf\", \".otf\", \".eot\"}\n",
    "binary_exts = {\".map\", \".zip\", \".exe\", \".bin\", \".dll\", \".so\", \".o\", \".gz\"}\n",
    "minified_exts = {\".min.js\", \".min.css\", \".js.map\", \".css.map\"}\n",
    "docs_exts = {\".pdf\", \".docx\", \".doc\", \".rtf\", \".odt\"}\n",
    "html_exts = {\".html\", \".htm\", \".xhtml\"}\n",
    "css_exts = {\".css\", \".scss\", \".sass\", \".less\"}\n",
    "xml_exts = {\".xml\", \".xsd\", \".xsl\"}\n",
    "\n",
    "print(\"Libraries imported and API keys loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36d930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying extensions for ModelEarth/localsite:\n",
      "Found extensions: {'', '.html', '.csv', '.gif', '.md', '.map', '.txt', '.woff2', '.svg', '.woff', '.xls', '.jpg', '.mdx', '.json', '.py', '.png', '.js', '.psd', '.css'}\n",
      "Unhandled extensions: {''}\n",
      ".json -> JSON/YAML chunking\n",
      ".png -> Image summary\n",
      ".js -> Code chunking (Python/JS/TS)\n",
      ".psd -> Image summary\n",
      ".mdx -> Markdown chunking\n",
      ".gif -> Image summary\n",
      ".css -> CSS chunking\n",
      ".jpg -> Image summary\n",
      ".woff -> Font summary\n",
      ".md -> Markdown chunking\n",
      ".map -> Binary/minified summary\n",
      ".txt -> Markdown chunking\n",
      ".woff2 -> Font summary\n",
      ".svg -> Image summary\n",
      ".html -> HTML chunking\n",
      ".py -> Code chunking (Python/JS/TS)\n",
      ".csv -> Data preview chunking\n",
      ".xls -> Data preview chunking\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define Repos and Verify Extensions\n",
    "REPOS = {\n",
    "    \"ModelEarth/modelearth\": \"/Users/lokesh/Desktop/Model_Earth/modelearth\"  # Update with actual path\n",
    "}\n",
    "\n",
    "def verify_extensions(repo_path):\n",
    "    found_extensions = set()\n",
    "    handled_extensions = code_exts | markdown_exts | data_exts | json_exts | image_exts | font_exts | binary_exts | minified_exts | docs_exts | html_exts | css_exts\n",
    "    for dirpath, _, filenames in os.walk(repo_path):\n",
    "        if \".git\" in dirpath.split(os.sep):\n",
    "            continue\n",
    "        for filename in filenames:\n",
    "            if filename == \".DS_Store\":\n",
    "                continue\n",
    "            ext = os.path.splitext(filename)[-1].lower()\n",
    "            found_extensions.add(ext)\n",
    "    unhandled = found_extensions - handled_extensions\n",
    "    print(f\"Found extensions: {found_extensions}\")\n",
    "    if unhandled:\n",
    "        print(f\"Unhandled extensions: {unhandled}\")\n",
    "    for ext in found_extensions & handled_extensions:\n",
    "        if ext in code_exts:\n",
    "            print(f\"{ext} -> Code chunking (Python/JS/TS)\")\n",
    "        elif ext in markdown_exts:\n",
    "            print(f\"{ext} -> Markdown chunking\")\n",
    "        elif ext in data_exts:\n",
    "            print(f\"{ext} -> Data preview chunking\")\n",
    "        elif ext in json_exts:\n",
    "            print(f\"{ext} -> JSON/YAML chunking\")\n",
    "        elif ext in image_exts:\n",
    "            print(f\"{ext} -> Image summary\")\n",
    "        elif ext in font_exts:\n",
    "            print(f\"{ext} -> Font summary\")\n",
    "        elif ext in binary_exts or ext in minified_exts:\n",
    "            print(f\"{ext} -> Binary/minified summary\")\n",
    "        elif ext in docs_exts:\n",
    "            print(f\"{ext} -> Document summary\")\n",
    "        elif ext in html_exts:\n",
    "            print(f\"{ext} -> HTML chunking\")\n",
    "        elif ext in css_exts:\n",
    "            print(f\"{ext} -> CSS chunking\")\n",
    "\n",
    "for repo_name, repo_path in REPOS.items():\n",
    "    if not os.path.exists(repo_path):\n",
    "        print(f\"Directory {repo_path} for {repo_name} does not exist. Skipping.\")\n",
    "        continue\n",
    "    print(f\"\\nVerifying extensions for {repo_name}:\")\n",
    "    verify_extensions(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Chunking files for ModelEarth/localsite\n",
      "‚úÖ Saved chunks to /Users/lokesh/Desktop/Model_Earth/rag-pipeline/ModelEarth_localsite_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tree_sitter import Language, Parser\n",
    "import uuid\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Tokenizer setup\n",
    "MAX_TOKENS = 8192\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def re_chunk_if_oversize(sections, max_tokens=MAX_TOKENS):\n",
    "    final_chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "\n",
    "        tokens = count_tokens(section)\n",
    "        if tokens <= max_tokens:\n",
    "            final_chunks.append(section)\n",
    "        else:\n",
    "            # Attempt semantic split first\n",
    "            split_points = re.split(r'(?<=[.!?])\\s+', section)\n",
    "            current_chunk = \"\"\n",
    "            for part in split_points:\n",
    "                if count_tokens(current_chunk + \" \" + part) <= max_tokens:\n",
    "                    current_chunk += \" \" + part\n",
    "                else:\n",
    "                    final_chunks.append(current_chunk.strip())\n",
    "                    current_chunk = part\n",
    "            if current_chunk.strip():\n",
    "                final_chunks.append(current_chunk.strip())\n",
    "\n",
    "            # Final hard split if still oversized\n",
    "            really_final = []\n",
    "            for chunk in final_chunks:\n",
    "                if count_tokens(chunk) <= max_tokens:\n",
    "                    really_final.append(chunk)\n",
    "                else:\n",
    "                    # Hard split on whitespace every ~3000 characters\n",
    "                    raw_splits = re.findall(r'.{1,3000}(?:\\s+|$)', chunk)\n",
    "                    really_final.extend([s.strip() for s in raw_splits if s.strip()])\n",
    "            final_chunks = really_final\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "# Supported extensions\n",
    "code_exts = {\".py\", \".js\", \".ts\", \".jsx\", \".tsx\", \".java\", \".cpp\", \".c\", \".cs\", \".go\", \".rb\", \".php\", \".rs\", \".swift\",\".kt\", \".kts\", \".scala\", \".cjs\", \".mjs\", \".ipynb\", \".sh\"}\n",
    "markdown_exts = {\".md\", \".mdx\", \".txt\", \".rst\", \".adoc\"}\n",
    "data_exts = {\".csv\", \".tsv\", \".xls\", \".xlsx\", \".parquet\", \".feather\", \".h5\", \".hdf5\"}\n",
    "json_exts = {\".json\", \".yaml\", \".yml\", \".jsonl\", \".webmanifest\"}\n",
    "image_exts = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".psd\", \".bmp\", \".tiff\", \".ico\"}\n",
    "font_exts = {\".woff\", \".woff2\", \".ttf\", \".otf\", \".eot\"}\n",
    "binary_exts = {\".map\", \".zip\", \".exe\", \".bin\", \".dll\", \".so\", \".o\", \".gz\"}\n",
    "minified_exts = {\".min.js\", \".min.css\", \".js.map\", \".css.map\"}\n",
    "docs_exts = {\".pdf\", \".docx\", \".doc\", \".rtf\", \".odt\"}\n",
    "html_exts = {\".html\", \".htm\", \".xhtml\"}\n",
    "css_exts = {\".css\", \".scss\", \".sass\", \".less\"}\n",
    "xml_exts = {\".xml\", \".xsd\", \".xsl\"}\n",
    "\n",
    "\n",
    "# Tree-sitter language support\n",
    "LANGUAGES = {\n",
    "    '.py': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'python'),\n",
    "    '.js': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'javascript'),\n",
    "    '.jsx': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'javascript'),\n",
    "    '.cjs': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'javascript'),\n",
    "    '.ts': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'typescript'),\n",
    "    '.java': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'java'),\n",
    "    '.cpp': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'cpp'),\n",
    "    '.c': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'c'),\n",
    "    '.cs': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'c_sharp'),\n",
    "    '.go': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'go'),\n",
    "    '.rb': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'ruby'),\n",
    "    '.php': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'php'),\n",
    "    '.rs': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'rust'),\n",
    "    '.swift': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'swift'),\n",
    "    '.html': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'html'),\n",
    "    '.css': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'css'),\n",
    "    '.xml': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'xml'),\n",
    "    '.scss': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'css'),\n",
    "    '.sh': Language('/Users/lokesh/Desktop/Model_Earth/tree-sitter/build/my-languages.so', 'bash'),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_repo_structure(repo_path: Path, repo_name: str) -> dict:\n",
    "    def build_compressed_tree(path: Path, base_path: Path) -> str:\n",
    "        # Collect unique directory paths and assign IDs\n",
    "        dir_map = {}\n",
    "        dir_counter = 1\n",
    "        structure = []\n",
    "        \n",
    "        for p in sorted(path.rglob(\"*\")):\n",
    "            relative_path = p.relative_to(base_path).as_posix()\n",
    "            parent_path = str(p.parent.relative_to(base_path)) if p != base_path else \"\"\n",
    "            parent_id = \"\"\n",
    "            \n",
    "            # Assign ID to parent directory if not already mapped\n",
    "            if parent_path and parent_path not in dir_map:\n",
    "                dir_map[parent_path] = f\"d{dir_counter}\"\n",
    "                dir_counter += 1\n",
    "            parent_id = dir_map.get(parent_path, \"\")\n",
    "            \n",
    "            if p.is_file():\n",
    "                # File: use parent ID and file name with extension\n",
    "                structure.append(f\"{parent_id}:{p.name}|{p.suffix or ''}\" if parent_id else f\"{p.name}|{p.suffix or ''}\")\n",
    "            else:\n",
    "                # Directory: only include if it has no children (leaf directory)\n",
    "                if not any(q.is_dir() for q in p.iterdir()):\n",
    "                    structure.append(f\"{parent_id}:{p.name}:\" if parent_id else f\"{p.name}:\")\n",
    "        \n",
    "        # Create ID map (e.g., \"d1:.git:hooks,d2:input:industries\")\n",
    "        id_map = \",\".join(f\"{k}:{v.replace('/', ':')}\" for v, k in dir_map.items())\n",
    "        # Join structure with semicolons\n",
    "        content = \";\".join(structure)\n",
    "        # Combine map and content\n",
    "        return f\"{id_map}|{content}\" if id_map else content\n",
    "\n",
    "    content = build_compressed_tree(repo_path, repo_path)\n",
    "    return {\n",
    "        \"repo_name\": repo_name,\n",
    "        \"file_path\": f\"{repo_name}/repo_structure\",\n",
    "        \"file_type\": \"structure\",\n",
    "        \"chunk_type\": \"repo_structure\",\n",
    "        \"chunk_id\": str(uuid.uuid4()),\n",
    "        \"embed\": True,\n",
    "        \"content\": content,\n",
    "        \"embedded\": False,\n",
    "        \"line_range\": \"L1-L1\"\n",
    "    }\n",
    "\n",
    "def chunk_code_tree_sitter(filepath, ext):\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGES[ext])\n",
    "    code = Path(filepath).read_text(encoding=\"utf-8\")\n",
    "    tree = parser.parse(code.encode(\"utf-8\"))\n",
    "    root = tree.root_node\n",
    "    chunks = []\n",
    "    for child in root.children:\n",
    "        if child.type in ['function_definition', 'class_definition', 'function', 'method_definition', 'element', 'style_rule']:\n",
    "            snippet = code[child.start_byte:child.end_byte]\n",
    "            chunks.append(snippet.strip())\n",
    "    return chunks if chunks else [code.strip()[:1000]]\n",
    "\n",
    "def chunk_ipynb(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
    "            notebook = json.load(f)\n",
    "        chunks = []\n",
    "        for i, cell in enumerate(notebook.get('cells', [])):\n",
    "            cell_type = cell.get('cell_type')\n",
    "            source = ''.join(cell.get('source', [])).strip()\n",
    "            if not source:\n",
    "                continue\n",
    "            if cell_type == 'markdown':\n",
    "                chunks.append(f\"[Markdown Cell {i+1}]\\n{source}\")\n",
    "            elif cell_type == 'code':\n",
    "                chunks.append(f\"[Code Cell {i+1}]\\n{source}\")\n",
    "            # Limit metadata to key info (e.g., kernel) to avoid bloat\n",
    "            if i == 0 and 'metadata' in notebook:\n",
    "                metadata = {k: v for k, v in notebook['metadata'].items() if k in ['kernelspec', 'language_info']}\n",
    "                chunks.append(f\"[Notebook Metadata]\\n{json.dumps(metadata, indent=2)}\")\n",
    "        return re_chunk_if_oversize(chunks)\n",
    "    except Exception as e:\n",
    "        return [f\"‚ö†Ô∏è Error parsing {filepath.name}: {e}\"]\n",
    "\n",
    "def chunk_html_tree_sitter_safe(filepath):\n",
    "    code = Path(filepath).read_text(encoding=\"utf-8\")\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGES['.html'])\n",
    "    tree = parser.parse(code.encode(\"utf-8\"))\n",
    "    root = tree.root_node\n",
    "    chunks = []\n",
    "\n",
    "    def collect_structural_nodes(node, code, max_tokens=MAX_TOKENS):\n",
    "        if node.type in ['script_element', 'style_element']:\n",
    "            return\n",
    "\n",
    "        if node.type == 'element' and node.named_child_count > 0 and node.start_byte != node.end_byte:\n",
    "            try:\n",
    "                tag_snippet = code[node.start_byte:node.start_byte + code[node.start_byte:].find('>') + 1]\n",
    "                tag_name = re.findall(r'<(\\w+)', tag_snippet)[0]\n",
    "            except Exception:\n",
    "                tag_name = \"\"\n",
    "\n",
    "            if tag_name in ['div', 'section', 'article', 'header', 'footer', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']:\n",
    "                snippet = code[node.start_byte:node.end_byte].strip()\n",
    "                if snippet:\n",
    "                    if count_tokens(snippet) <= max_tokens:\n",
    "                        chunks.append(snippet)\n",
    "                    else:\n",
    "                        sub_chunks = re_chunk_if_oversize([snippet])\n",
    "                        chunks.extend(sub_chunks)\n",
    "\n",
    "        for child in node.children:\n",
    "            collect_structural_nodes(child, code)\n",
    "\n",
    "    # Traverse root tree\n",
    "    for child in root.children:\n",
    "        collect_structural_nodes(child, code)\n",
    "\n",
    "    # Fallback in case nothing was collected\n",
    "    if not chunks:\n",
    "        fallback = code.strip()\n",
    "        if count_tokens(fallback) <= MAX_TOKENS:\n",
    "            chunks.append(fallback)\n",
    "        else:\n",
    "            chunks.extend(re_chunk_if_oversize([fallback]))\n",
    "\n",
    "    # üõ°Ô∏è Final safety net\n",
    "    safe_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if count_tokens(chunk) <= MAX_TOKENS:\n",
    "            safe_chunks.append(chunk)\n",
    "        else:\n",
    "            safe_chunks.extend(re_chunk_if_oversize([chunk]))\n",
    "\n",
    "    return safe_chunks\n",
    "\n",
    "\n",
    "def chunk_xml(filepath):\n",
    "    code = Path(filepath).read_text(encoding=\"utf-8\")\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGES['.xml'])\n",
    "    tree = parser.parse(code.encode(\"utf-8\"))\n",
    "    root = tree.root_node\n",
    "    chunks = []\n",
    "\n",
    "    def collect_structural_nodes(node, code, max_tokens=MAX_TOKENS):\n",
    "        if node.type == 'element' and node.named_child_count > 0 and node.start_byte != node.end_byte:\n",
    "            snippet = code[node.start_byte:node.end_byte].strip()\n",
    "            if snippet and count_tokens(snippet) <= max_tokens:\n",
    "                chunks.append(snippet)\n",
    "            elif snippet:\n",
    "                sub_chunks = re_chunk_if_oversize([snippet])\n",
    "                chunks.extend(sub_chunks)\n",
    "        for child in node.children:\n",
    "            collect_structural_nodes(child, code)\n",
    "\n",
    "    for child in root.children:\n",
    "        collect_structural_nodes(child, code)\n",
    "    \n",
    "    if not chunks:\n",
    "        chunks.append(code.strip()[:1000])\n",
    "    \n",
    "    return re_chunk_if_oversize(chunks)\n",
    "\n",
    "def chunk_markdown(filepath):\n",
    "    text = Path(filepath).read_text(encoding=\"utf-8\")\n",
    "    sections, current = [], []\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith(\"#\") and current:\n",
    "            sections.append(\"\\n\".join(current).strip())\n",
    "            current = []\n",
    "        current.append(line)\n",
    "    if current:\n",
    "        sections.append(\"\\n\".join(current).strip())\n",
    "    return sections\n",
    "\n",
    "def chunk_json_yaml(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
    "            data = yaml.safe_load(f) if filepath.suffix in {'.yaml', '.yml'} else json.load(f)\n",
    "        \n",
    "        chunks = []\n",
    "\n",
    "        def chunk_value(value, parent_key=None):\n",
    "            text = json.dumps(value, indent=2)\n",
    "            if count_tokens(text) <= MAX_TOKENS:\n",
    "                chunks.append(text)\n",
    "            else:\n",
    "                # Try chunking dicts by keys\n",
    "                if isinstance(value, dict):\n",
    "                    for k, v in value.items():\n",
    "                        chunk_value({k: v}, parent_key=k)\n",
    "                elif isinstance(value, list):\n",
    "                    for i, item in enumerate(value):\n",
    "                        chunk_value(item, parent_key=f\"{parent_key or 'list'}[{i}]\")\n",
    "                else:\n",
    "                    # fallback for unbreakable types\n",
    "                    chunks.append(text)\n",
    "\n",
    "        chunk_value(data)\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        return [f\"‚ö†Ô∏è Error parsing {filepath.name}: {e}\"]\n",
    "\n",
    "\n",
    "\n",
    "def chunk_csv_xls(filepath: Path):\n",
    "    try:\n",
    "        suffix = filepath.suffix.lower()\n",
    "        if suffix in {'.xls', '.xlsx'}:\n",
    "            df = pd.read_excel(filepath, nrows=5)\n",
    "        elif suffix in {'.csv', '.tsv'}:\n",
    "            df = pd.read_csv(filepath, sep=None, engine='python', nrows=5, encoding='utf-8', encoding_errors='ignore')\n",
    "        elif suffix == '.parquet':\n",
    "            df = pd.read_parquet(filepath)\n",
    "            df = df.head(5)\n",
    "        elif suffix == '.feather':\n",
    "            df = pd.read_feather(filepath)\n",
    "            df = df.head(5)\n",
    "        elif suffix == '.pkl':\n",
    "            df = pd.read_pickle(filepath)\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                return [f\"‚ö†Ô∏è Pickle file does not contain a DataFrame: {filepath.name}\"]\n",
    "            df = df.head(5)\n",
    "        else:\n",
    "            return [f\"‚ö†Ô∏è Unsupported file format: {suffix}\"]\n",
    "\n",
    "        if df.empty:\n",
    "            return [f\"‚ö†Ô∏è Empty DataFrame in {filepath.name}\"]\n",
    "\n",
    "        columns = \", \".join(df.columns)\n",
    "        preview = df.to_string(index=False)\n",
    "        chunk = f\"Columns: {columns}\\nPreview (first 5 rows):\\n{preview}\"\n",
    "        return [chunk] if chunk.strip() else [f\"‚ö†Ô∏è Empty chunk for {filepath.name}\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback to raw text read\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()[:5]\n",
    "                chunk = f\"Raw content (first 5 lines):\\n{''.join(lines).strip()}\"\n",
    "                return [chunk] if chunk.strip() else [f\"‚ö†Ô∏è Empty file content for {filepath.name}\"]\n",
    "        except Exception as e2:\n",
    "            return [f\"‚ö†Ô∏è Error reading {filepath.name}: {e2}\"]\n",
    "\n",
    "\n",
    "def chunk_as_summary(filepath: Path, root_folder: str = \"Model_Earth\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a summary chunk with a clean relative path starting from the project root folder.\n",
    "\n",
    "    Example output:\n",
    "    ['PDF file at Model_Earth/docs/report.pdf']\n",
    "    \"\"\"\n",
    "    # Convert to absolute path and POSIX-style string\n",
    "    full_path = filepath.resolve().as_posix()\n",
    "\n",
    "    # Find the path from root_folder onward\n",
    "    if root_folder in full_path:\n",
    "        relative_path = full_path.split(root_folder, 1)[-1]\n",
    "        clean_path = f\"{root_folder}{relative_path}\"\n",
    "    else:\n",
    "        clean_path = filepath.name  # fallback if root_folder not found\n",
    "\n",
    "    # Format as summary\n",
    "    return [f\"{filepath.suffix.upper()[1:]} file at {clean_path}\"]\n",
    "\n",
    "\n",
    "def dispatch_chunking(file):\n",
    "    ext = file.suffix.lower()\n",
    "    try:\n",
    "        if ext in code_exts:\n",
    "            if ext == '.ipynb':\n",
    "                chunks = chunk_ipynb(file)\n",
    "                embed = True\n",
    "            else:\n",
    "                chunks = chunk_code_tree_sitter(file, ext)\n",
    "                embed = True\n",
    "        elif ext in html_exts:\n",
    "            chunks = chunk_html_tree_sitter_safe(file)\n",
    "            embed = True\n",
    "        elif ext in css_exts:\n",
    "            chunks = chunk_code_tree_sitter(file, ext)\n",
    "            embed = True\n",
    "        elif ext in xml_exts:\n",
    "            chunks = chunk_xml(file)\n",
    "            embed = True\n",
    "        elif ext in markdown_exts:\n",
    "            chunks = chunk_markdown(file)\n",
    "            embed = True\n",
    "        elif ext in json_exts:\n",
    "            chunks = chunk_json_yaml(file)\n",
    "            embed = True\n",
    "        elif ext in data_exts:\n",
    "            chunks = chunk_csv_xls(file)\n",
    "            embed = True\n",
    "        elif ext in image_exts | font_exts | binary_exts | minified_exts | docs_exts:\n",
    "            chunks = chunk_as_summary(file)\n",
    "            embed = True\n",
    "        else:\n",
    "            summary = chunk_as_summary(file)\n",
    "            return [(chunk.strip(), True) for chunk in summary if chunk.strip()]\n",
    "\n",
    "\n",
    "        chunks = re_chunk_if_oversize(chunks) if embed else chunks\n",
    "        return [(chunk, embed) for chunk in chunks]\n",
    "    except Exception as e:\n",
    "        return [(f\"‚ö†Ô∏è Error parsing {file.name}: {e}\", False)]\n",
    "\n",
    "def get_line_range(content, full_text):\n",
    "    if not full_text or not content:  # For summaries or non-text files\n",
    "        return \"L1-L1\"  # Treat summary as single-line\n",
    "    try:\n",
    "        start_idx = full_text.find(content)\n",
    "        if start_idx == -1:\n",
    "            return \"L1-L1\"  # Fallback for summaries or unmatched content\n",
    "        before = full_text[:start_idx]\n",
    "        start_line = before.count(\"\\n\") + 1\n",
    "        line_count = content.count(\"\\n\") + 1\n",
    "        end_line = start_line + line_count - 1\n",
    "        return f\"L{start_line}-L{end_line}\"\n",
    "    except:\n",
    "        return \"L1-L1\"  # Fallback for any errors\n",
    "\n",
    "def collect_all_valid_files(repo_path: Path, allowed_exts: set) -> list:\n",
    "    return [file for file in repo_path.rglob(\"*\") if file.is_file() and file.suffix.lower() in allowed_exts]\n",
    "\n",
    "# Main execution\n",
    "chunks_dict = defaultdict(list)\n",
    "\n",
    "for repo_name, repo_path in REPOS.items():\n",
    "    print(f\"\\nüîç Chunking files for {repo_name}\")\n",
    "    path = Path(repo_path)\n",
    "    valid_files = [file for file in path.rglob(\"*\") if file.is_file()]\n",
    "    repo_chunks = [generate_repo_structure(path, repo_name)]\n",
    "\n",
    "    for file in valid_files:\n",
    "        ext = file.suffix.lower()\n",
    "        try:\n",
    "            full_text = file.read_text(encoding=\"utf-8\", errors=\"ignore\") \\\n",
    "                        if ext not in (image_exts | font_exts | binary_exts | minified_exts | docs_exts) else \"\"\n",
    "        except:\n",
    "            full_text = \"\"\n",
    "        chunk_data = dispatch_chunking(file)\n",
    "        for i, (chunk, embed) in enumerate(chunk_data):\n",
    "            # Skip only if chunk is not a string\n",
    "            if not isinstance(chunk, str):\n",
    "                print(f\"‚ö†Ô∏è Skipping non-string chunk from {file.name}\")\n",
    "                continue\n",
    "\n",
    "            chunk_entry = {\n",
    "                \"repo_name\": repo_name,\n",
    "                \"file_path\": f\"{repo_name}/{file.relative_to(path)}\",\n",
    "                \"file_type\": ext.lstrip('.'),\n",
    "                \"chunk_type\": \"summary\" if not embed else \"content\",\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"embed\": embed,\n",
    "                \"content\": chunk.strip(),\n",
    "                \"embedded\": False,\n",
    "                \"line_range\": get_line_range(chunk, full_text)\n",
    "            }\n",
    "            repo_chunks.append(chunk_entry)\n",
    "\n",
    "    output_path = Path(OUTPUT_DIR) / f\"{repo_name.replace('/', '_')}_chunks.jsonl\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in repo_chunks:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    chunks_dict[repo_name] = repo_chunks\n",
    "    print(f\"‚úÖ Saved chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139a0eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Final Chunk Stats for ModelEarth/localsite\n",
      "   ‚û§ Total chunks: 940\n",
      "   ‚û§ Marked for embedding (embed=True): 940\n",
      "   ‚û§ Skipped from embedding (embed=False): 0\n",
      "   ‚û§ Empty content chunks (embed=True): 0\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Count what‚Äôs actually going into repo_chunks\n",
    "total_chunks = len(repo_chunks)\n",
    "embeddable = sum(1 for c in repo_chunks if c[\"embed\"])\n",
    "non_embeddable = sum(1 for c in repo_chunks if not c[\"embed\"])\n",
    "empty_chunks = sum(1 for c in repo_chunks if not c[\"content\"].strip())\n",
    "print(f\"üì¶ Final Chunk Stats for {repo_name}\")\n",
    "print(f\"   ‚û§ Total chunks: {total_chunks}\")\n",
    "print(f\"   ‚û§ Marked for embedding (embed=True): {embeddable}\")\n",
    "print(f\"   ‚û§ Skipped from embedding (embed=False): {non_embeddable}\")\n",
    "print(f\"   ‚û§ Empty content chunks (embed=True): {empty_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3be2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üö® Skipped Chunks (embed=False): 0\n"
     ]
    }
   ],
   "source": [
    "# Final debug print to catch skipped chunks\n",
    "skipped_chunks = [c for c in repo_chunks if not c[\"embed\"]]\n",
    "print(f\"\\nüö® Skipped Chunks (embed=False): {len(skipped_chunks)}\")\n",
    "for sc in skipped_chunks:\n",
    "    print(f\"‚õî {sc['file_path']} | type={sc['file_type']} | content={sc['content'][:60]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299beab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking: ModelEarth_localsite_chunks.jsonl\n",
      "‚úÖ All chunks are within token limits.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Setup tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "MAX_TOKENS = 8192\n",
    "\n",
    "# Directory where chunked files are saved\n",
    "CHUNK_DIR = Path(OUTPUT_DIR)  # or replace with actual path\n",
    "\n",
    "# Scan all jsonl files in output dir\n",
    "for jsonl_file in CHUNK_DIR.glob(\"_chunks.jsonl\"):\n",
    "    print(f\"\\nüîç Checking: {jsonl_file.name}\")\n",
    "    over_limit_chunks = []\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            tokens = len(tokenizer.encode(obj[\"content\"])) if obj.get(\"content\") else 0\n",
    "            if tokens > MAX_TOKENS:\n",
    "                over_limit_chunks.append((obj[\"file_path\"], obj[\"chunk_id\"], tokens))\n",
    "\n",
    "    if over_limit_chunks:\n",
    "        print(f\"‚ùå Found {len(over_limit_chunks)} chunks over {MAX_TOKENS} tokens:\")\n",
    "        for path, chunk_id, count in over_limit_chunks:\n",
    "            print(f\"  - {path} | Chunk ID: {chunk_id} | Tokens: {count}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All chunks are within token limits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Embedding chunks for: ModelEarth/localsite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding ModelEarth/localsite: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 940/940 [03:17<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedded 940 chunks for ModelEarth/localsite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DIMENSION = 1536  # Dimension for text-embedding-3-small\n",
    "METRIC = \"cosine\"\n",
    "BATCH_SIZE = 10 \n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "for repo_name, chunks in chunks_dict.items():\n",
    "    print(f\"\\nüîç Embedding chunks for: {repo_name}\")\n",
    "    repo_embeddings = []\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=f\"Embedding {repo_name}\"):\n",
    "        if chunk.get(\"embed\", False) and not chunk.get(\"embedded\", False):\n",
    "            try:\n",
    "                # Generate embedding\n",
    "                response = openai_client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=chunk[\"content\"]\n",
    "                )\n",
    "                vector = response.data[0].embedding\n",
    "\n",
    "                # Mark as embedded and store embedding with content\n",
    "                chunk[\"embedded\"] = True\n",
    "                repo_embeddings.append({\n",
    "                    \"id\": chunk[\"chunk_id\"],\n",
    "                    \"values\": vector,\n",
    "                    \"metadata\": {\n",
    "                        \"repo_name\": chunk[\"repo_name\"],\n",
    "                        \"file_path\": chunk[\"file_path\"],\n",
    "                        \"file_type\": chunk[\"file_type\"],\n",
    "                        \"chunk_type\": chunk[\"chunk_type\"],\n",
    "                        \"line_range\": chunk[\"line_range\"],\n",
    "                        \"content\": chunk[\"content\"]  # Include content for Pinecone\n",
    "                    }\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to embed chunk {chunk['chunk_id']}: {e}\")\n",
    "\n",
    "    embeddings_dict[repo_name] = repo_embeddings\n",
    "    print(f\"‚úÖ Embedded {len(repo_embeddings)} chunks for {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc2d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 940, To embed: 940\n"
     ]
    }
   ],
   "source": [
    "total = len(chunks_dict[\"ModelEarth/localsite\"])\n",
    "to_embed = sum(1 for c in chunks_dict[\"ModelEarth/localsite\"] if c.get(\"embed\", False))\n",
    "print(f\"Total chunks: {total}, To embed: {to_embed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c150b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pinecone index: model-earth-jam-stack\n",
      "\n",
      "üöÄ Uploading embeddings for repo: ModelEarth/localsite (namespace: ModelEarth_localsite)\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded batch of 20 embeddings for ModelEarth/localsite\n",
      "‚úÖ Uploaded 940 vectors for ModelEarth/localsite into namespace 'ModelEarth_localsite'\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"model-earth-jam-stack\"\n",
    "METRIC = \"cosine\"\n",
    "DIMENSION = 1536 \n",
    "\n",
    "# Create or access index\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    print(f\"Creating Pinecone index: {INDEX_NAME}\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=DIMENSION,\n",
    "        metric=METRIC,\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# Connect to index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Upsert embeddings per repository in batches\n",
    "for repo_name, records in embeddings_dict.items():\n",
    "    namespace = repo_name.replace(\"/\", \"_\")  # Safe namespace\n",
    "    print(f\"\\nüöÄ Uploading embeddings for repo: {repo_name} (namespace: {namespace})\")\n",
    "    if not records:\n",
    "        print(f\"‚ö†Ô∏è No embeddings to upload for {repo_name}\")\n",
    "        continue\n",
    "\n",
    "    total_uploaded = 0\n",
    "    for i in range(0, len(records), BATCH_SIZE):\n",
    "        batch = records[i:i + BATCH_SIZE]\n",
    "        vectors = [\n",
    "            {\n",
    "                \"id\": record[\"id\"],\n",
    "                \"values\": record[\"values\"],\n",
    "                \"metadata\": {\n",
    "                    \"repo_name\": record[\"metadata\"].get(\"repo_name\", \"\"),\n",
    "                    \"file_path\": record[\"metadata\"].get(\"file_path\", \"\"),\n",
    "                    \"file_type\": record[\"metadata\"].get(\"file_type\", \"\"),\n",
    "                    \"chunk_type\": record[\"metadata\"].get(\"chunk_type\", \"unknown\"),\n",
    "                    \"line_range\": record[\"metadata\"].get(\"line_range\", \"\"),\n",
    "                    \"content\": record[\"metadata\"].get(\"content\", \"\")\n",
    "                }\n",
    "            }\n",
    "            for record in batch\n",
    "            if record.get(\"id\") and record.get(\"values\") and record[\"metadata\"].get(\"content\")\n",
    "        ]\n",
    "\n",
    "        if not vectors:\n",
    "            print(f\"‚ö†Ô∏è Skipping invalid batch at index {i} for {repo_name} (no valid embeddings)\")\n",
    "            continue\n",
    "\n",
    "        # Check metadata size (Pinecone limit: 40KB)\n",
    "        for vector in vectors:\n",
    "            metadata_size = len(json.dumps(vector[\"metadata\"]).encode(\"utf-8\"))\n",
    "            if metadata_size > 40000:\n",
    "                print(f\"‚ö†Ô∏è Truncating content for chunk {vector['id']} (size: {metadata_size} bytes)\")\n",
    "                vector[\"metadata\"][\"content\"] = vector[\"metadata\"][\"content\"][:10000]\n",
    "\n",
    "        try:\n",
    "            index.upsert(vectors=vectors, namespace=namespace)\n",
    "            total_uploaded += len(vectors)\n",
    "            print(f\"‚úÖ Uploaded batch of {len(vectors)} embeddings for {repo_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Batch failed at index {i} for {repo_name}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Uploaded {total_uploaded} vectors for {repo_name} into namespace '{namespace}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8a24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60219f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f340500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208a663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f0cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cf2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495902d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9354f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72e7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
